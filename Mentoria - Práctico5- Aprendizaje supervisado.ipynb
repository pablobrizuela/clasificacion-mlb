{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos todas las librerias necesarias en el notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import pandas\n",
    "import seaborn\n",
    "import sys\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el dataset de Mercado-libre. La columna TITLE y DOMAIN_ID lo leemos como string. Para ATTRIBUTES usamos CustomParser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499948"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = pandas.read_csv('meli_dataset_20190426.csv', converters={'ATTRIBUTES':str,'DOMAIN_ID': str, 'TITLE': str})\n",
    "row0 = dataset.shape[0]\n",
    "row0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>SHP_WEIGHT</th>\n",
       "      <th>SHP_LENGTH</th>\n",
       "      <th>SHP_WIDTH</th>\n",
       "      <th>SHP_HEIGHT</th>\n",
       "      <th>ATTRIBUTES</th>\n",
       "      <th>CATALOG_PRODUCT_ID</th>\n",
       "      <th>CONDITION</th>\n",
       "      <th>DOMAIN_ID</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>SELLER_ID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>TITLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M1CQ76ZT5W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>H53U1H7Q5G</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>404</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SN7ISIGQ9J</td>\n",
       "      <td>235.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[{'id': 'BRAND', 'name': 'Marca', 'value_id': ...</td>\n",
       "      <td>H53U1H7Q5G</td>\n",
       "      <td>new</td>\n",
       "      <td>MLB-SKIN_CARE_SUPPLIES</td>\n",
       "      <td>68.0</td>\n",
       "      <td>QF4OJMYQ9Q</td>\n",
       "      <td>active</td>\n",
       "      <td>Ácido Hidroquinona 20%   30g + Sabonete Pré Pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JGEV50GW2U</td>\n",
       "      <td>1757.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>[{'id': 'ACCESSORIES_INCLUDED', 'name': 'Acess...</td>\n",
       "      <td>YRBDJR6T7Y</td>\n",
       "      <td>new</td>\n",
       "      <td>MLB-NEBULIZERS</td>\n",
       "      <td>145.9</td>\n",
       "      <td>WEE71CZC2Q</td>\n",
       "      <td>active</td>\n",
       "      <td>Inalador E Nebulizador Infantil Nebdog Superfl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JGEV50GW2U</td>\n",
       "      <td>1748.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>[{'id': 'ACCESSORIES_INCLUDED', 'name': 'Acess...</td>\n",
       "      <td>YRBDJR6T7Y</td>\n",
       "      <td>new</td>\n",
       "      <td>MLB-NEBULIZERS</td>\n",
       "      <td>145.9</td>\n",
       "      <td>WEE71CZC2Q</td>\n",
       "      <td>active</td>\n",
       "      <td>Inalador E Nebulizador Infantil Nebdog Superfl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JGEV50GW2U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'id': 'ACCESSORIES_INCLUDED', 'name': 'Acess...</td>\n",
       "      <td>YRBDJR6T7Y</td>\n",
       "      <td>new</td>\n",
       "      <td>MLB-NEBULIZERS</td>\n",
       "      <td>145.9</td>\n",
       "      <td>WEE71CZC2Q</td>\n",
       "      <td>active</td>\n",
       "      <td>Inalador E Nebulizador Infantil Nebdog Superfl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ITEM_ID  SHP_WEIGHT  SHP_LENGTH  SHP_WIDTH  SHP_HEIGHT  \\\n",
       "0  M1CQ76ZT5W         NaN         NaN        NaN         NaN   \n",
       "1  SN7ISIGQ9J       235.0        25.0       25.0        10.0   \n",
       "2  JGEV50GW2U      1757.0        23.0       17.0        16.0   \n",
       "3  JGEV50GW2U      1748.0        23.0       17.0        16.0   \n",
       "4  JGEV50GW2U         NaN         NaN        NaN         NaN   \n",
       "\n",
       "                                          ATTRIBUTES CATALOG_PRODUCT_ID  \\\n",
       "0                                                            H53U1H7Q5G   \n",
       "1  [{'id': 'BRAND', 'name': 'Marca', 'value_id': ...         H53U1H7Q5G   \n",
       "2  [{'id': 'ACCESSORIES_INCLUDED', 'name': 'Acess...         YRBDJR6T7Y   \n",
       "3  [{'id': 'ACCESSORIES_INCLUDED', 'name': 'Acess...         YRBDJR6T7Y   \n",
       "4  [{'id': 'ACCESSORIES_INCLUDED', 'name': 'Acess...         YRBDJR6T7Y   \n",
       "\n",
       "  CONDITION               DOMAIN_ID  PRICE   SELLER_ID  STATUS  \\\n",
       "0       NaN                            NaN         NaN     404   \n",
       "1       new  MLB-SKIN_CARE_SUPPLIES   68.0  QF4OJMYQ9Q  active   \n",
       "2       new          MLB-NEBULIZERS  145.9  WEE71CZC2Q  active   \n",
       "3       new          MLB-NEBULIZERS  145.9  WEE71CZC2Q  active   \n",
       "4       new          MLB-NEBULIZERS  145.9  WEE71CZC2Q  active   \n",
       "\n",
       "                                               TITLE  \n",
       "0                                                     \n",
       "1  Ácido Hidroquinona 20%   30g + Sabonete Pré Pe...  \n",
       "2  Inalador E Nebulizador Infantil Nebdog Superfl...  \n",
       "3  Inalador E Nebulizador Infantil Nebdog Superfl...  \n",
       "4  Inalador E Nebulizador Infantil Nebdog Superfl...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos valores cuyo status es `404` , luego eliminamos la columna `status` del dataset ya que solo es útil para limpieza.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = dataset[ dataset['STATUS'] == '404' ].index\n",
    "dataset.drop(indices , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78361"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1 = dataset.shape[0]\n",
    "row0-row1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.380061510190018"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row0/(row0-row1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que 78361 filas fueron removidas. Esto es un 6 porciento de data set original de casi 500 mil filas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eliminamos los valores NaN de las columnas con prefijo `SHP_`. Estas son aquellas que representan o peso o dimensiones de un item.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = dataset[dataset['SHP_WEIGHT'].isna() | dataset['SHP_LENGTH'].isna() | \n",
    "                  dataset['SHP_WIDTH'].isna() | dataset['SHP_HEIGHT'].isna() ].index\n",
    "dataset.drop(indices , inplace=True)\n",
    "row2 = dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nos quedan  296325  filas. Fueron removidas  125262 . Esto es un  2.365641615174594 porciento.\n"
     ]
    }
   ],
   "source": [
    "print('Nos quedan ',row2,' filas. Fueron removidas ',row1-row2, '. Esto es un ',row2/(row1-row2),'porciento.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agrupamos por item id y calcular mediana de peso y medidas. De esta forma nos queda una única fila por cada item_id.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "listColumns = list(dataset.columns)\n",
    "columns_dic={}\n",
    "for item in listColumns:\n",
    "    if item.startswith('SHP_'):\n",
    "        columns_dic[item] = 'median'\n",
    "    else:\n",
    "        columns_dic[item] = 'first'\n",
    "dataset = dataset.groupby('ITEM_ID').agg(columns_dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SHP_WEIGHT</th>\n",
       "      <th>SHP_LENGTH</th>\n",
       "      <th>SHP_WIDTH</th>\n",
       "      <th>SHP_HEIGHT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>236443.00000</td>\n",
       "      <td>236443.000000</td>\n",
       "      <td>236443.000000</td>\n",
       "      <td>236443.000000</td>\n",
       "      <td>2.106590e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1818.96622</td>\n",
       "      <td>31.398151</td>\n",
       "      <td>21.251572</td>\n",
       "      <td>11.503536</td>\n",
       "      <td>1.958341e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3199.67595</td>\n",
       "      <td>18.469790</td>\n",
       "      <td>11.157975</td>\n",
       "      <td>8.234870</td>\n",
       "      <td>6.638232e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>250.00000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.790000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>650.00000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.999000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1883.75000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.799000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50000.00000</td>\n",
       "      <td>288.200000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>3.032487e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SHP_WEIGHT     SHP_LENGTH      SHP_WIDTH     SHP_HEIGHT         PRICE\n",
       "count  236443.00000  236443.000000  236443.000000  236443.000000  2.106590e+05\n",
       "mean     1818.96622      31.398151      21.251572      11.503536  1.958341e+03\n",
       "std      3199.67595      18.469790      11.157975       8.234870  6.638232e+05\n",
       "min         1.00000       0.000000       0.000000       0.000000  1.000000e-01\n",
       "25%       250.00000      20.000000      13.000000       5.000000  4.790000e+01\n",
       "50%       650.00000      25.000000      20.000000      10.000000  9.999000e+01\n",
       "75%      1883.75000      36.000000      25.000000      16.000000  1.799000e+02\n",
       "max     50000.00000     288.200000     115.000000     105.000000  3.032487e+08"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4-Parsear la columna de atributos y extraer a columnas propias aquellos atributos cuyo `id` sea `BRAND` o `MODEL`. Estos atributos representan marca o modelo que el vendedor del item ingresó en la publicación. [Opcional] No es necesario limitarse a estos dos atributos, se puede probar quedarse con los N atributos más frecuentes.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demora aproximadamente 10 minutos en parsear el dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def parse_attributes(row):\n",
    "    if row['ATTRIBUTES'] == '':\n",
    "        return row['DOMAIN_ID'] + ' ' + row['TITLE']\n",
    "    data_dict = ast.literal_eval(row['ATTRIBUTES'])\n",
    "    #print(data_dict)\n",
    "    data_df = pandas.DataFrame.from_dict(data_dict)\n",
    "    if not 'id' in data_df.columns:\n",
    "        return row['DOMAIN_ID'] + ' ' + row['TITLE']\n",
    "    brand_value_name = data_df[data_df['id']=='BRAND']['value_name'].values\n",
    "    model_value_name = data_df[data_df['id']=='MODEL']['value_name'].values \n",
    "    \n",
    "    if brand_value_name.size == 1:\n",
    "        brand_value_name = brand_value_name[0]\n",
    "        if brand_value_name is None:\n",
    "            brand_value_name = ''\n",
    "    else:\n",
    "        brand_value_name = ''\n",
    "    if model_value_name.size == 1:\n",
    "        model_value_name = model_value_name[0]\n",
    "        if model_value_name is None:\n",
    "            model_value_name = ''\n",
    "    else:\n",
    "        model_value_name = ''\n",
    "    return brand_value_name + ' ' + model_value_name + ' ' + row['DOMAIN_ID'] + ' ' + row['TITLE']\n",
    " \n",
    "#dataset.sample(50).apply(parse_attributes, axis=1)\n",
    "dataset['X'] = dataset.apply(parse_attributes, axis=1)\n",
    "#dataset2 = dataset.apply(parse_attributes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th>SHP_WEIGHT</th>\n",
       "      <th>SHP_LENGTH</th>\n",
       "      <th>SHP_WIDTH</th>\n",
       "      <th>SHP_HEIGHT</th>\n",
       "      <th>ATTRIBUTES</th>\n",
       "      <th>CATALOG_PRODUCT_ID</th>\n",
       "      <th>CONDITION</th>\n",
       "      <th>DOMAIN_ID</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>SELLER_ID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ITEM_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A002DG7EAZ</th>\n",
       "      <td>A002DG7EAZ</td>\n",
       "      <td>812.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td></td>\n",
       "      <td>H53U1H7Q5G</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>U2T0EY02XB</td>\n",
       "      <td>under_review</td>\n",
       "      <td>Apresentador Multimídia Wireless Logitech R400...</td>\n",
       "      <td>Apresentador Multimídia Wireless Logitech R40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A00SG33UIH</th>\n",
       "      <td>A00SG33UIH</td>\n",
       "      <td>2320.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[{'id': 'AUTHOR', 'name': 'Autor', 'value_id':...</td>\n",
       "      <td>H53U1H7Q5G</td>\n",
       "      <td>new</td>\n",
       "      <td>MLB-BOOKS</td>\n",
       "      <td>149.99</td>\n",
       "      <td>BCZWFNME44</td>\n",
       "      <td>active</td>\n",
       "      <td>Apostila Trt-sp 2018 - Analista Jud.  Área Apo...</td>\n",
       "      <td>MLB-BOOKS Apostila Trt-sp 2018 - Analista Ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A00VIC9XL7</th>\n",
       "      <td>A00VIC9XL7</td>\n",
       "      <td>213.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[{'id': 'BRAND', 'name': 'Marca', 'value_id': ...</td>\n",
       "      <td>H53U1H7Q5G</td>\n",
       "      <td>new</td>\n",
       "      <td>MLB-BABY_GROOMING_KITS</td>\n",
       "      <td>329.00</td>\n",
       "      <td>T2JY69NPBA</td>\n",
       "      <td>active</td>\n",
       "      <td>Wetstop 3 Alarme Miccional Xixi Na Cama Enurese</td>\n",
       "      <td>Wet Stop 3+ MLB-BABY_GROOMING_KITS Wetstop 3 A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A00VM7MP9F</th>\n",
       "      <td>A00VM7MP9F</td>\n",
       "      <td>175.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>[{'id': 'ITEM_CONDITION', 'name': 'Condição do...</td>\n",
       "      <td>H53U1H7Q5G</td>\n",
       "      <td>new</td>\n",
       "      <td>MLB-BICYCLE_BOTTLE_CAGES</td>\n",
       "      <td>45.00</td>\n",
       "      <td>YA6XOOJU39</td>\n",
       "      <td>active</td>\n",
       "      <td>Suporte De Garrafa Zefal Wiiz Para Bicicleta</td>\n",
       "      <td>zefal wiiz MLB-BICYCLE_BOTTLE_CAGES Suporte De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A00W1VSE3K</th>\n",
       "      <td>A00W1VSE3K</td>\n",
       "      <td>82.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[{'id': 'ALARM', 'name': 'Com alarme', 'value_...</td>\n",
       "      <td>H53U1H7Q5G</td>\n",
       "      <td>new</td>\n",
       "      <td>MLB-PEDOMETERS</td>\n",
       "      <td>31.98</td>\n",
       "      <td>DCLDPQAY43</td>\n",
       "      <td>active</td>\n",
       "      <td>Relógio Marcador De Passos Distancia E Caloria...</td>\n",
       "      <td>BLUELANS Led MLB-PEDOMETERS Relógio Marcador D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ITEM_ID  SHP_WEIGHT  SHP_LENGTH  SHP_WIDTH  SHP_HEIGHT  \\\n",
       "ITEM_ID                                                                 \n",
       "A002DG7EAZ  A002DG7EAZ       812.0        36.0       32.0        12.0   \n",
       "A00SG33UIH  A00SG33UIH      2320.0        16.0       11.0         4.0   \n",
       "A00VIC9XL7  A00VIC9XL7       213.0        16.0       13.0        10.0   \n",
       "A00VM7MP9F  A00VM7MP9F       175.0        25.0       20.0        15.0   \n",
       "A00W1VSE3K  A00W1VSE3K        82.0        30.0       15.0         5.0   \n",
       "\n",
       "                                                   ATTRIBUTES  \\\n",
       "ITEM_ID                                                         \n",
       "A002DG7EAZ                                                      \n",
       "A00SG33UIH  [{'id': 'AUTHOR', 'name': 'Autor', 'value_id':...   \n",
       "A00VIC9XL7  [{'id': 'BRAND', 'name': 'Marca', 'value_id': ...   \n",
       "A00VM7MP9F  [{'id': 'ITEM_CONDITION', 'name': 'Condição do...   \n",
       "A00W1VSE3K  [{'id': 'ALARM', 'name': 'Com alarme', 'value_...   \n",
       "\n",
       "           CATALOG_PRODUCT_ID CONDITION                 DOMAIN_ID   PRICE  \\\n",
       "ITEM_ID                                                                     \n",
       "A002DG7EAZ         H53U1H7Q5G       NaN                               NaN   \n",
       "A00SG33UIH         H53U1H7Q5G       new                 MLB-BOOKS  149.99   \n",
       "A00VIC9XL7         H53U1H7Q5G       new    MLB-BABY_GROOMING_KITS  329.00   \n",
       "A00VM7MP9F         H53U1H7Q5G       new  MLB-BICYCLE_BOTTLE_CAGES   45.00   \n",
       "A00W1VSE3K         H53U1H7Q5G       new            MLB-PEDOMETERS   31.98   \n",
       "\n",
       "             SELLER_ID        STATUS  \\\n",
       "ITEM_ID                                \n",
       "A002DG7EAZ  U2T0EY02XB  under_review   \n",
       "A00SG33UIH  BCZWFNME44        active   \n",
       "A00VIC9XL7  T2JY69NPBA        active   \n",
       "A00VM7MP9F  YA6XOOJU39        active   \n",
       "A00W1VSE3K  DCLDPQAY43        active   \n",
       "\n",
       "                                                        TITLE  \\\n",
       "ITEM_ID                                                         \n",
       "A002DG7EAZ  Apresentador Multimídia Wireless Logitech R400...   \n",
       "A00SG33UIH  Apostila Trt-sp 2018 - Analista Jud.  Área Apo...   \n",
       "A00VIC9XL7    Wetstop 3 Alarme Miccional Xixi Na Cama Enurese   \n",
       "A00VM7MP9F       Suporte De Garrafa Zefal Wiiz Para Bicicleta   \n",
       "A00W1VSE3K  Relógio Marcador De Passos Distancia E Caloria...   \n",
       "\n",
       "                                                            X  \n",
       "ITEM_ID                                                        \n",
       "A002DG7EAZ   Apresentador Multimídia Wireless Logitech R40...  \n",
       "A00SG33UIH    MLB-BOOKS Apostila Trt-sp 2018 - Analista Ju...  \n",
       "A00VIC9XL7  Wet Stop 3+ MLB-BABY_GROOMING_KITS Wetstop 3 A...  \n",
       "A00VM7MP9F  zefal wiiz MLB-BICYCLE_BOTTLE_CAGES Suporte De...  \n",
       "A00W1VSE3K  BLUELANS Led MLB-PEDOMETERS Relógio Marcador D...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorizamos la columna X utilizando TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('portuguese')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pbrizuela/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demora aproximadamente 5 minutos en realizar el fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=<function text_process at 0x7f2d23a1fbf8>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.float64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=None, smooth_idf=True, stop_words=None,\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(dataset['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180452"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform(dataset['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector[vector>0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=(dataset['SHP_LENGTH']>70) | (dataset['SHP_WIDTH'] >70) | (dataset['SHP_HEIGHT'] >70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236443"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10434"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.sum(labels==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226009"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.sum(labels==False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-Splitear el dataset en train/test (80-20). Recordar la utilidad train_test_split de scikit-learn. Utilizar los parámetros `random_state` y `stratify` y explicar su utilidad.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, label_train, label_test = train_test_split(vector, labels, test_size = 0.2, random_state=0, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189154"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8347"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.sum(label_train==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180807"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.sum(label_train==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47289"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2087"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.sum(label_test==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45202"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.sum(label_test==False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parametro random_state se utiliza para inicializar las semillas aleatorias y que el resultado de su ejecucion sea reproducible.\n",
    "El parametro stratify se utiliza para que la separacion de entre train y split tengan las misma distribucion en sus clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-Entrenar y evaluar con al menos 3 nuevos modelos (Sugerencias: SVM, RandomForest, GradientBoostingClassifier, etc.) Obligatorio: Probar con una red neuronal. Puede ser de scikit-learn o de alguna otra librería que deseen como  keras, pytorch, etc.). Junto con las métricas debe entregarse una breve descripción de cómo funciona cada modelo.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos un data frame para almacenar la mejor metrica obtenida para cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "results = pandas.DataFrame(columns=('clf', 'best_acc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos Grid Search para encontrar el mejor modelo para NB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best NB scoring:  0.01918120986748998\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "Seconds: 2.6763265132904053\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "start_time = time.time()\n",
    "nb_param = {\n",
    "    'alpha': [1.0],\n",
    "}\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb_clf = GridSearchCV(nb, nb_param, scoring='f1', cv=5, iid=False, n_jobs=-1)\n",
    "nb_clf.fit(data_train, label_train)\n",
    "\n",
    "best_nb_clf = nb_clf.best_estimator_\n",
    "print('Best NB scoring: ', nb_clf.best_score_)\n",
    "print(best_nb_clf)\n",
    "results = results.append({'clf': best_nb_clf , 'best_acc': nb_clf.best_score_}, ignore_index=True)\n",
    "print(f'Seconds: {time.time() - start_time}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      1.00      0.98     45202\n",
      "        True       0.87      0.02      0.04      2087\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     47289\n",
      "   macro avg       0.91      0.51      0.51     47289\n",
      "weighted avg       0.95      0.96      0.94     47289\n",
      "\n",
      "[[45195     7]\n",
      " [ 2039    48]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "y_pred = best_nb_clf.predict(data_test)\n",
    "print(classification_report(label_test,y_pred))\n",
    "print(confusion_matrix(label_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos Grid Search para encontrar el mejor modelo para Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "start_time = time.time()\n",
    "perceptron_param = {\n",
    "    'alpha': [1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_perceptron = Perceptron(tol=1e-3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Perceptron scoring:  0.46704596501424633\n",
      "Perceptron(alpha=1.0, class_weight=None, early_stopping=False, eta0=1.0,\n",
      "      fit_intercept=True, max_iter=None, n_iter=None, n_iter_no_change=5,\n",
      "      n_jobs=None, penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
      "      validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "Seconds: 2.4606375694274902\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(tol=1e-3, random_state=0)\n",
    "perceptron_clf = GridSearchCV(perceptron, perceptron_param, scoring='f1', cv=5, iid=False, n_jobs=-1)\n",
    "perceptron_clf.fit(data_train, label_train)\n",
    "\n",
    "best_perceptron_clf = perceptron_clf.best_estimator_\n",
    "print('Best Perceptron scoring: ', perceptron_clf.best_score_)\n",
    "print(best_perceptron_clf)\n",
    "results = results.append({'clf': best_nb_clf , 'best_acc': nb_clf.best_score_}, ignore_index=True)\n",
    "print(f'Seconds: {time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.98      0.98     45202\n",
      "        True       0.48      0.47      0.48      2087\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     47289\n",
      "   macro avg       0.73      0.73      0.73     47289\n",
      "weighted avg       0.95      0.95      0.95     47289\n",
      "\n",
      "[[44108  1094]\n",
      " [ 1096   991]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_perceptron_clf.predict(data_test)\n",
    "print(classification_report(label_test,y_pred))\n",
    "print(confusion_matrix(label_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos Grid Search para encontrar el mejor modelo para SVM. Probamos con dos kernel y con diferentes class_weight y que en nuestro caso tenemos clases desbalanceadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM scoring:  0.5530504104544234\n",
      "LinearSVC(C=1.0, class_weight={True: 3}, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "The best classifier so far is: \n",
      "LinearSVC(C=1.0, class_weight={True: 3}, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Seconds: 25.507173538208008\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "start_time = time.time()\n",
    "svm_param = {\n",
    "    'class_weight': [{True:1},{True:2},{True:3},{True:4},{True:5}],\n",
    "}\n",
    "\n",
    "svm = LinearSVC()\n",
    "svm_clf = GridSearchCV(svm, svm_param, scoring='f1', cv=5, iid=False, n_jobs=-1)\n",
    "svm_clf.fit(data_train, label_train)\n",
    "\n",
    "best_svm_clf = svm_clf.best_estimator_\n",
    "print('Best SVM scoring: ', svm_clf.best_score_)\n",
    "print(best_svm_clf)\n",
    "results = results.append({'clf': best_svm_clf , 'best_acc': svm_clf.best_score_}, ignore_index=True)\n",
    "\n",
    "print('The best classifier so far is: ')\n",
    "print(results.loc[results['best_acc'].idxmax()]['clf'])\n",
    "print(f'Seconds: {time.time() - start_time}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.98      0.98     45202\n",
      "        True       0.59      0.55      0.57      2087\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     47289\n",
      "   macro avg       0.78      0.77      0.77     47289\n",
      "weighted avg       0.96      0.96      0.96     47289\n",
      "\n",
      "[[44387   815]\n",
      " [  935  1152]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "y_pred = best_svm_clf.predict(data_test)\n",
    "print(classification_report(label_test,y_pred))\n",
    "print(confusion_matrix(label_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hacemos word2vect para reducir dimensionalidad (TBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from gensim.sklearn_api import W2VTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences=[]\n",
    "for index, row in dataset.iterrows():\n",
    "    tokenized= []\n",
    "    for word in row['X'].split(' '):\n",
    "        word = word.split('.')[0]\n",
    "        word = word.lower()\n",
    "        tokenized.append(word)\n",
    "    sentences.append(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = W2VTransformer(size=100, min_count=1, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordvecs = model.fit(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordvecs.transform(['alarme','wetstop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos Grid Search para encontrar el mejor modelo para Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "rfc_param = {\n",
    "    'n_estimators': [10,15],\n",
    "    'criterion': ['gini'],\n",
    "#    'max_depth': [1, 10, 100, 1000],\n",
    "#    'min_samples_split': [2, 5, 10, 100],\n",
    "#    'min_samples_leaf': [1, 2, 5, 10, 100]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbrizuela/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/home/pbrizuela/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/home/pbrizuela/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/home/pbrizuela/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest scoring:  0.4610420361745356\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=-1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "The best classifier so far is: \n",
      "LinearSVC(C=1.0, class_weight={True: 3}, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Seconds: 2281.7930777072906\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "rfc_clf = GridSearchCV(rfc, rfc_param, scoring='f1', cv=5, iid=False, n_jobs=-1)\n",
    "rfc_clf.fit(data_train, label_train)\n",
    "\n",
    "best_rfc_clf = rfc_clf.best_estimator_\n",
    "print('Best Random Forest scoring: ', rfc_clf.best_score_)\n",
    "print(best_rfc_clf)\n",
    "results = results.append({'clf': best_rfc_clf , 'best_acc': rfc_clf.best_score_}, ignore_index=True)\n",
    "\n",
    "print('The best classifier so far is: ')\n",
    "print(results.loc[results['best_acc'].idxmax()]['clf'])\n",
    "print(f'Seconds: {time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.99      0.98     45202\n",
      "        True       0.74      0.35      0.47      2087\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     47289\n",
      "   macro avg       0.86      0.67      0.73     47289\n",
      "weighted avg       0.96      0.97      0.96     47289\n",
      "\n",
      "[[44951   251]\n",
      " [ 1364   723]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_rfc_clf.predict(data_test)\n",
    "print(classification_report(label_test,y_pred))\n",
    "print(confusion_matrix(label_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos Grid Search para encontrar el mejor modelo para Multi Layer Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_param = {\n",
    "    'hidden_layer_sizes': [(10, 10, 10),(20, 20, 20),(30, 30, 30)],\n",
    "    'activation': ['relu']\n",
    "}\n",
    "\n",
    "mlp = MLPClassifier(activation='relu', solver='adam', alpha=1e-5, hidden_layer_sizes=(30, 30, 30), random_state=1)\n",
    "mlp_clf = GridSearchCV(mlp, mlp_param, scoring='f1', cv=5, iid=False, n_jobs=-1)\n",
    "mlp_clf.fit(data_train, label_train)\n",
    "\n",
    "best_mlp_clf = mlp_clf.best_estimator_\n",
    "print('Best MLP scoring: ', mlp_clf.best_score_)\n",
    "print(best_mlp_clf)\n",
    "results = results.append({'clf': best_rfc_clf , 'best_acc': rfc_clf.best_score_}, ignore_index=True)\n",
    "\n",
    "print('The best classifier so far is: ')\n",
    "print(results.loc[results['best_acc'].idxmax()]['clf'])\n",
    "print(f'Seconds: {time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_mlp_clf.predict(data_test)\n",
    "print(classification_report(label_test,y_pred))\n",
    "print(confusion_matrix(label_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3-Para estos nuevos modelos tunear hiper-parámetros. Para las evaluaciones utilizar la técnica de k-fold cross-validation (ver cross-validation) y explicar los resultados.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, label_train, label_test = train_test_split(vector, labels, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4- Elegir el mejor modelo de scikit-learn entrenado hasta el momento según f1-score e implementar una función `predict_with_threshold(model, X, threshold)` tal que si la probabilidad dada por el método `model.predict_proba(X)` es mayor a `threshold` entonces la clase es positiva (no maquinable) y negativa en caso contrario. Notar que predict_with_threshold(model, X, 0.5) debería ser equivalente a `model.predict(X)`. Luego evaluar el mismo modelo para distintos thresholds 0.3, 0.4, 0.5, 0.6, 0.7. Reportar métricas e interpretar los resultados..**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_threshold(model, X, threshold):\n",
    "    return (model.predict_proba(X)[:,1] > threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de LinearSVM no contamos con el metodo predict_proba. Para obtener las probabilidades se puede usar\n",
    "CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "cclf = CalibratedClassifierCV(base_estimator=LinearSVC(penalty='l2', dual=False), cv=5)\n",
    "cclf.fit(data_train, label_train)\n",
    "res = cclf.predict_proba(data_test)[:, 1];\n",
    "#an array containing probabilities of belonging to the 1st class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = cclf\n",
    "y_true = label_test\n",
    "for threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]: \n",
    "    y_pred = predict_with_threshold(model, data_test, threshold)\n",
    "    print(threshold)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el mejor modelo segun el F1 score si tenemos disponible la funcion predict_proba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cclf\n",
    "y_true = label_test\n",
    "for threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]: \n",
    "    y_pred = predict_with_threshold(model, data_test, threshold)\n",
    "    print(threshold)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
